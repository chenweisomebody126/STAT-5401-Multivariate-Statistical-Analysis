###Assign9      
### Wei Chen 5207679      
Q7.3      
Let the $Y= Z+ \beta +\varepsilon$ , where $E(\varepsilon)=0$ but $E(\varepsilon\varepsilon')=\sigma^{2}V, with V(n\times n)$ known and positive difinite. For V of full rank, show that the weighted least squares estimator is $\hat{\beta}_{w}$= $(Z'V^{-1}Z)^{-1}Z'V^{-1}Y$. If $\sigma^{2}$ is unknown, it may be estimated, unbiasedly by $(n-r-1)^{-1} \times (Y-Z\hat{\beta}_{w})'V^{-1}(Y-Z\hat{\beta}_{w})$.      
Solution:  
If we left multiply $V^{-1/2}$ to both sides of linear regression, we get $V^{-1/2}Y=$ $(V^{-1/2}Z)\beta +V^{-1/2}\varepsilon$. Let $Y^{*}$ be $V^{-1/2}Y=$,  $Z^{*}$ be $(V^{-1/2}Z)$ and  $\varepsilon^{*}$ be $V^{-1/2}\varepsilon$. And then we get $Y^{*} = Z^{*}\beta_{w} + \varepsilon^{*}$. Because V is positive definite with full rank,  $E(\varepsilon^{*})=E(V^{-1/2}\varepsilon)=0$ and $E(\varepsilon^{*}\varepsilon^{*}$')=$E(V^{-1/2}\varepsilon\varepsilon V^{-1/2})$=$V^{-1/2}E(\varepsilon\varepsilon')V^{-1/2}=\sigma^{2}I$. For the classical linear regression $Y^{*} = Z^{*}\beta_{w} + \varepsilon^{*}$, $\hat{\beta}_{w}$=$(Z^{*}$$'Z^{*})^{-1}$$Z^{*}$$'Y^{*}$=$((V^{-1/2})'$$(ZV^{1/2}Z))^{-1}(V^{-1/2}Z)'(V^{-1/2}Y)$=$(Z'V^{-1}Z)^{-1}Z'V^{-1}Y$. Also, if we do not know $\sigma^{2}$, $\hat{\sigma}^{2}=Var(\varepsilon^{*})$=$E(\frac{1}{n-r-1}\varepsilon^{*}\varepsilon^{*}$$'$)=$(n-r-1)^{-1} \times (V^{-1/2}\varepsilon)'(V^{-1/2}\varepsilon)$=$(n-r-1)^-{-1} \times (Y-Z\hat{\beta}_{w})'V^{-1}(Y-Z\hat{\beta}_{w})$.     

Q7.4    
Use the weighted least squares estimator in Exercise 7.3 to derive for the estimate of the slope $\beta$ in the model $Y_{j}= \beta z_{j}+\varepsilon_{j},j=1,2,...n$, when (a) $Var(\varepsilon_{j})=\sigma^{2}$,(b)$Var(\varepsilon_{j})=\sigma^{2} z_{j}$, (c)$Var(\varepsilon_{j})=\sigma^{2} z_{j}^{2}$. Comment on the manner in which the unequal variances for the errors influence the optimal choice of $\hat{\beta}_{w})$.      
(a)In this case, V is a Indentity matrix I.       
So the $\hat{\beta}_{w}$=$(Z'Z)^{-1}Z'Y$=$\sum_{j}z_{j}y_{j}/\sum_{i}z_{i}^{2}$.     
(b)V is the diagonal matrix with jth diagonal entry $z_{j},j=1,..n$.             $\hat{\beta}_{w}$=$(Z'Z^{-1}Z)^{-1}Z'V^{-1}Y$=$\sum_{j}y_{j}/\sum_{i}z_{i}$.      
(c)V is a diagnal matrix with jth diagonal entry $z_{j}^{2},j=1,..n$.     
$\hat{\beta}_{w}$=$(Z'Z^{-1}Z)^{-1}Z'V^{-1}Y$=$(\sum_{j}y_{j}/z_{j})/n$

Q7.8      
(a)Since $H=Z(Z'Z)^{-1}Z'$, $HH=Z(Z'Z)^{-1}Z'Z(Z'Z)^{-1}Z'=ZI(Z'Z)^{-1}Z=Z(Z'Z)^{-1}Z=H$. So H is idempotent matrix.     
(b)Since $(Z'Z)$ is symmetric, $(Z'Z)^{-1}$$'(Z'Z)=$$(Z'Z)^{-1}$$'(Z'Z)'=I$. So $(Z'Z)^{-1}$$'=(Z'Z)^{-1}$. $H'=(Z(Z'Z)^{-1}Z')'$=$Z((Z'Z)^{-1})$$'Z'=Z(Z'Z)^{-1}Z'=H$, so H is also symmetric matrix. According to $HH=H$, $h_{jj}=h_{jj}^{2}+\sum_{i\neq j}h_{ij}h_{ji}$. Because H is symmetrix, $h_{ij}=h_{ji}$, so $h_{jj}=h_{jj}^{2}+\sum_{i\neq j}h_{ij}^{2}\ge0$. Since $\sum_{i\neq j}h_{ij}^{2}\ge0$, $h_{jj}\ge h_{jj}^{2}$.       
We get $h_{jj}^{2}-h_{jj}=h_{jj}(h_{jj}-1) \le0$, because $h_{jj}\le 0$, we get $h_{jj}\le1$.      
Q7.17     
Consider the Forbes data in Exercise 1.4.
(a)Fit a linear regression model to these data using profits as the dependent variable and sales and assets as the independent variables.      
(b)Analyze the residuals to check the adequacy of the model. Compute the leverages associated with the data points. Does one(or more) of these companies stand out as outlier in the set of independent variable data points?     
(c)Generate a 95% prediction interval for profits corresponding to sales of 100(billions of dollars) and assets of 500(billions of dollars).      
(d)Carry out a likelihood ratio test of H~0~:$\beta_{2}=0$ with a significance level of $\alpha=0.05$. Should the original model be modified? Discuss.      
Solution:       
(a)
```{r}
URL="http://users.stat.umn.edu/~guxxx192/courses/wichern_data/p1-4.dat"
forbes<-read.table(URL,col.names=c("sales","profits","assets"))
lm_fit <- lm(profits ~ sales + assets, data = forbes)
summary(lm_fit)
```
(b)    
Calculate the studentized residuals
```{r}
rstudent(lm_fit)
```
Compute the leverage and the Cook's distance to find potential outlier(s)
```{r}
hatvalues(lm_fit)
cooks.distance(lm_fit)
```
We find that all leverages are less than 3(r+1)/n=0.9, which indicates no outliers.
      
(c)     
```{r}
alpha=0.05
n<-nrow(forbes)
r<-2
Z = as.matrix(cbind(1.0, forbes[, c(1,3)]))
Y = as.vector(forbes[, 2])
H = Z %*% solve(t(Z) %*% Z) %*% t(Z)
beta_hat <-as.vector(solve(t(Z) %*% Z) %*% t(Z) %*% Y)
s2<-t(Y) %*% (diag(n) - H) %*% Y/(n - r - 1)
z0<-as.vector(c(1,100,500))
center<-t(z0)%*%beta_hat
radius<-sqrt(s2*(1+t(z0)%*%solve(t(Z)%*%Z)%*%z0))
print(lower<-center-qt(alpha/2, df = n - r - 1, lower.tail = FALSE) *radius)
print(upper<-center+qt(alpha/2, df = n - r - 1, lower.tail = FALSE) *radius)
```
(d)     
We first compute the $\hat{\Sigma}$
```{r}
alpha<-0.05
Sigma_hat<-t(Y-Z%*%beta_hat)%*%(Y-Z%*%beta_hat)/n
```
We run the regression under null hypothesis and calculate $\hat{\Sigma_{0}}$
```{r}
Z0 = as.matrix(cbind(1.0, forbes[, 1]))
H0 = Z0 %*% solve(t(Z0) %*% Z0) %*% t(Z0)
beta0_hat <- solve(t(Z0) %*% Z0) %*% t(Z0) %*% Y
Sigma0_hat<-t(Y-Z0%*%beta0_hat)%*%(Y-Z0%*%beta0_hat)/n
```
Then we can build the test statistic which, according to Result7.11 when n is large, should approximately follows the chi square distribution
```{r}
m<-1
q<-1
print(statis<- -(n-r-1-0.5*(m-r+q+1))*log(det(Sigma_hat)/det(Sigma0_hat)))
print(threshold<- qchisq(alpha, df=m*(r-q), lower.tail = FALSE))
```
Because we can not reject the null hypothesis: $\beta_{2}=0$, we should remove the $\beta_{2}$.