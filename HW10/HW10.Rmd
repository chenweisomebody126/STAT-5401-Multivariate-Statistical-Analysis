---
title: "HW10"
output: html_document
---

7.18 (p. 424),
Calculate     
(a) a C~p~ plot corresponding to the possible regressions involving the Forbes data in Excersice 1.4    
(b) the AIC for each possible regression.     
Solution:         
(a)
```{r}
URL="http://users.stat.umn.edu/~guxxx192/courses/wichern_data/p1-4.dat"
forbes<-read.table(URL,col.names=c("sales","profits","assets"))
```
Cp plot
```{r}
library(leaps)
Cpplot <- function(cp, ...) {
  p <- max(cp$size)
  plot(cp$size, cp$Cp, xlab = "p", ylab = "Cp", type = "n")
  labels <- apply(
    cp$which, 1,
    function(x) {
      if (any(x) == FALSE) return("(0)")
      paste0("(", paste(as.character((1:(p - 1))[x]), collapse = ", "), ")")
    }
  )
  points(cp$size, cp$Cp, pch=16)
  text(cp$size, cp$Cp, labels, ...)
  abline(0, 1)
}
```
exhaustive search for all possible subset models
```{r}
exhaust <- leaps(forbes[, c(1,3)], forbes[, 2])
```
calculate Cp for the full model
```{r}
lm_fit <- lm(profits ~ sales + assets, data = forbes)
```
calculate Cp for the intercept model
```{r}
fit0 = lm(profits ~ 1, data = forbes)
Cp0 = sum(fit0$residuals^2)/(summary(lm_fit)$sigma^2) -(nrow(forbes) - 2 * 1)
exhaust$which = rbind(rep(FALSE,2), exhaust$which)
rownames(exhaust$which)[1] = "0"
exhaust$size = c(1, exhaust$size)
exhaust$Cp = c(Cp0, exhaust$Cp)
Cpplot(exhaust, pos = c(1, 4, 1, 3), cex = 0.7)
```
      
(b)AIC     
```{r}
AIC(lm_fit)
AIC(lm(profits ~ sales, data = forbes))
AIC(lm(profits ~ assets, data = forbes))
AIC(fit0)
```
7.9 (p. 422),     
Consider the following data n one predictor variable z~1~ and two responses Y~1~ and Y~2~. Determinie the least squares estimate of the parameters in the bivariate straight-line regression model.     
Solution:     
```{r}
z1 = c(-2,-1,0,1,2)
y1 = c(5,3,4,2,1)
y2 = c(-3,-1,-1,2,3)
ex79data = data.frame(y1=y1, y2=y2, z1=z1)
## fit multivariate linear model
fit = lm(cbind(y1, y2) ~ z1, data=ex79data)
fitted(fit) # fitted values
fit$residuals # residuals
## verify decomposition
t(cbind(y1,y2)) %*% cbind(y1,y2)
t(fitted(fit)) %*% fitted(fit) + t(fit$residuals) %*% fit$residuals
```
7.11 (p. 422),      
Let A be a positive definite matrix, so that d~j~^2^(B) =(y~j~-B'z~j~)'A(y~j~-B'z~j~) is a squared statistical distance from the jth observation y~j~ to its regression B'z~j~. Show that the choice $B=\hat{\beta}=(Z'Z)^{-1}Z'Y$ minimizes the sum of squared statistical distances, $Sum_{j=1}^{n}d_{j}^{2}(B)$, for any choice of positive definite A. Choices for A include $\Sigma^{-1}$ and I.      
Solution:     
Since A is positive definite,
$RSS(B)=\sum_{j=1}^{n}d_{j}^{2}(B)=\sum_{j=1}^{n}(y_{j}-B'z_{j})'A(y_{j}-B'z_{j})=(Y-ZB)'A(Y-ZB)=tr((Y-ZB)'(Y-ZB)A)=Y'AY-2B'AZ'Y+B'AZ'ZB$.      
According to the porperty of trace ,
Because $\frac{\partial RSS(B)}{\partial B}= -2AZ'Y+2AZ'ZB=0$, then $AZ'ZB=AZ'Y$, so $\hat{\beta}=(AZ'Z)^{-1}AZ'Y=(Z'Z)^{-1}(Z'Y)$.      
A can be any choice if positive definite including $\Sigma^{-1}$ and $I$.     
7.25 (p. 426),      
(a)Perform a regression analysis using only the first response Y~1~.      
(i)Suggest and fit appropriate linear reression models
(ii)Analyze the residuals.      
(iii)Constuct a 95% prediction interval for total TCAD for z~1~=1, z~2~=1200, z~3~=140, z~4~=70 and z~5~=85.      
(b)Repeat Part a using the second response Y~2~.      
(c)Perform a multivariate multiple regression analysis using both response Y~1~ and Y~2~.      
(i)Suggest and fit appropriate linear reression models
(ii)Analyze the residuals.      
(iii)Constuct a 95% prediction interval for total TCAD and Amount of amitriptyline for z~1~=1, z~2~=1200, z~3~=140, z~4~=70 and z~5~=85.Compare the ellipse with prediction intervals in Parts a and b. Comment.     
Solution:     
(a)
(i)import the data and fatorize the z~1~ as binary variable.
```{r}
ami = read.table("http://users.stat.umn.edu/~guxxx192/courses/wichern_data/t7-6.dat",
                 col.names=c(paste0("y", 1:2), paste0("z", 1:5)))
ami$z1 = as.factor(ami$z1) # geneder variable is a factor
```
I used ANOVA to select variable and found that only z~2~ has significant effect.      
```{r}
library(car)
y1aov=aov(y1 ~z1+z2+z3+z4+z5, data = ami)
summary(y1aov)
```
(ii)Look at the Residuals
```{r}
y1fit=lm(y1~z2,data=ami)
summary(y1fit)
residualPlots(y1fit)
```
(iii)
 prediction interval for the mean function at a new observation z~2~=1200
```{r}
print(y1fitpred<-predict(y1fit, newdata = data.frame(z2=1200), interval = "predict"))
```
(b)
I used ANOVA to select variable and found that only z~1~ and z~2~ has significant effect.      
```{r}
y2aov=aov(y2 ~z1+z2+z3+z4+z5, data = ami)
summary(y2aov)
```
(ii)Look at the Residuals
```{r}
y2fit=lm(y2~z1+z2,data=ami)
summary(y2fit)
residualPlots(y2fit)
```
(iii)
 prediction interval for the mean function at a new observation z~1~=1,z~2~=1200
```{r}
print(y2fitpred<-predict(y2fit, newdata = data.frame(z1=as.factor(1),z2=1200), interval = "predict"))
```
(c)     
(i)I used MANOVA to select variable and found that only z~1~ and z~2~ has significant effect.
```{r}
y1y2fit = lm(cbind(y1, y2) ~ ., data = ami)
Manova(y1y2fit)
```
After we remove variables other than z~1~ and z~2~, we apply MANOVA again.
```{r}
fit_updated = lm(cbind(y1, y2) ~ z1+z2, data = ami)
Manova(fit_updated)
```
(ii)Look at the residuals
```{r}
plot(residuals(fit_updated)[,1])
plot(residuals(fit_updated)[,2])
```
(iii)
```{r}
z0<-data.frame(1,as.factor(1), 1200)
Z = cbind(1, ami$z1, ami$z2)
gram_inv = solve(t(Z) %*% Z)
predvals = predict(fit_updated, newdata = data.frame(z1=z0[1,2], z2 = z0[1,3]))
```
draw the prediction ellipse
```{r}
z0<-c(1,1,1200)
n=nrow(ami)
m=2
r=2
Sigma_hat = t(fit_updated$residuals) %*% fit_updated$residuals/n
alpha = 0.05
radius_pred = drop(sqrt((1+ t(z0) %*% gram_inv %*% z0) *m * (n - r - 1)/(n - r- m) *qf(1-alpha, df1=m, df2=n-r-m)))
ellipse(center=as.vector(predvals), shape=n/(n-r-1)*Sigma_hat,radius=radius_pred, add=FALSE,  asp = 1) 
```
      
Compare three Parts, we can see 
```{r}
ellipse(center=as.vector(predvals), shape=n/(n-r-1)*Sigma_hat,radius=radius_pred, add=FALSE,  asp = 1) 
abline(h=y2fitpred[2])
abline(h=y2fitpred[3])
abline(v=y1fitpred[2])
abline(v=y1fitpred[3])
```
      
The ellipse for y1, y2 in Part 3 is larger than interval for y1 in Part 1 and interval for y2 in Part 2
7.26 (p. 427)
(a)Perform a regression analysising using each of the response variables Y~1~,Y~2~,Y~3~ and Y~4~.     
(i)Suggest and fit appropriate linear regression models     
(ii) Analyze the residuals.     
(iii)Construct a 95%prediction interval     
(b)Perform a multivaraite multiple regression analysis using all four variables and four independent variables.      
(i)Suggest and fit appropriate linear regression models     
(ii) Analyze the residuals.     
(iii)Construct a 95%prediction interval     
Solution:     
(a)     
(i)First import the data
```{r}
URL="http://users.stat.umn.edu/~guxxx192/courses/wichern_data/t7-7.dat"
fiber<-read.table(URL,col.names=c(paste0("y", 1:4), paste0("z", 1:4)))
```
FOr each response variable y
```{r}
summary(aov(y1 ~z1+z2+z3+z4, data = fiber))
summary(aov(y2 ~z1+z2+z3+z4, data = fiber))
summary(aov(y3 ~z1+z2+z3+z4, data = fiber))
summary(aov(y4 ~z1+z2+z3+z4, data = fiber))
```
(ii)Look at the Residuals
```{r}
residualPlots(lm(y1~z1+z2+z4,data=fiber))
residualPlots(lm(y2~z1+z2+z3+z4,data=fiber))
residualPlots(lm(y3~z1+z2+z4,data=fiber))
residualPlots(lm(y4~z1+z3+z4,data=fiber))
```
(iii)
 prediction interval for the mean function at a new observation z~1~=0.33,z~2~=45.5 and z~4~=1.01
```{r}
y3fit<-lm(y3~z1+z2+z4,data=fiber)
print(y3pred<-predict(y3fit, newdata = data.frame(z1=0.33,z2=1200,z4=1.01), interval = "predict"))
```
(b)     
(i)I apply MANOVA
```{r}
y1to4fit = lm(cbind(y1,y2,y3,y4) ~ ., data = fiber)
Manova(y1to4fit)
```
Choose z~2~,z~3~ and z~4~ to update the model
```{r}
y1to4fit_updated = lm(cbind(y1,y2,y3,y4) ~ z2+z3+z4, data = fiber)
print(beta_hat<-coefficients(y1to4fit_updated))
epls=residuals(y1to4fit_updated)
n=nrow(epls)
r=3
print(sigma_hat<-(t(epls)%*%epls)/(n-r-1))
```
(ii)
```{r}
par(mfrow = c(2,2))
plot(epls[,1])
plot(epls[,2])
plot(epls[,3])
plot(epls[,4])
```

(iii)
```{r}
z0<-c(1,45.5,20.375,1.01)
Z = cbind(1, fiber$z2, fiber$z3,fiber$z4)
predvals = predict(y1to4fit_updated, newdata = data.frame(z2=z0[2], z3= z0[3], z4=z0[4]))
gram_inv = solve(t(Z) %*% Z)
radius_pred = drop(sqrt((1+ t(z0)%*% gram_inv %*% z0 )*m* (n - r - 1)/(n - r- m) *qf(1-alpha, df1=m, df2=n-r-m)))
```
calculate the prediction interval
```{r}
print(y1low<-predvals[1]-radius_pred*sqrt(sigma_hat[1,1]*n/(n-r-1)) )
print(y1upp<-predvals[1]+radius_pred*sqrt(sigma_hat[1,1]*n/(n-r-1)) )
print(y2low<-predvals[2]-radius_pred*sqrt(sigma_hat[2,2]*n/(n-r-1)) )
print(y2upp<-predvals[2]+radius_pred*sqrt(sigma_hat[2,2]*n/(n-r-1)) )
print(y3low<-predvals[3]-radius_pred*sqrt(sigma_hat[3,3]*n/(n-r-1)) )
print(y3upp<-predvals[3]+radius_pred*sqrt(sigma_hat[3,3]*n/(n-r-1)) )
print(y4low<-predvals[4]-radius_pred*sqrt(sigma_hat[4,4]*n/(n-r-1)) )
print(y4upp<-predvals[4]+radius_pred*sqrt(sigma_hat[4,4]*n/(n-r-1)) )
```

We can see that simultaneous interval for y3 is wider than interval in Part a.
